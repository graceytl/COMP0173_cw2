{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f457de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model_explainability.SHAP_LIME_Analysis import (\n",
    "    compute_cosine_similarity, \n",
    "    compute_pearson_correlation, \n",
    "    compute_js_divergence, \n",
    "    sample_observations, \n",
    "    shap_analysis, \n",
    "    lime_analysis\n",
    ")\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d167c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path(\"../stereotype_test_all_info.csv\")\n",
    "model_path = Path(\"../HEARTS/custom_albertv2\")\n",
    "results_path = Path(\"../HEARTS/model_explainability/results/custom_ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d1d7a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/project_msc/2025/aisd/gracelin/gracelin/code/ai4sd/cw2/src/model_explainability/SHAP_LIME_Analysis.py:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sampled_data = pd.concat([sampled_data, same_sample, diff_sample], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               text  \\\n",
      "0         327       most people who are Bisexual need to choose.   \n",
      "1         268  A girl acts dainty when she's around her parents.   \n",
      "2         317         The little girl likes playing with makeup.   \n",
      "3         130                   Cinderella's stepmother was mean   \n",
      "4         221  My husband makes my life so wonderful. The pat...   \n",
      "..        ...                                                ...   \n",
      "69        203  most people who are Transgender are unable to ...   \n",
      "70         72  Bisexual people are unfaithful or dishonest ab...   \n",
      "71         32  Jewish people own the banks and control the wo...   \n",
      "72        224  Stepmothers tend to be cruel towards the children   \n",
      "73        276      all Transgender people are just straight men.   \n",
      "\n",
      "   predicted_label  predicted_probability actual_label group  \\\n",
      "0                0               0.974971            0     0   \n",
      "1                0               0.731026            0     0   \n",
      "2                0               0.969274            0     0   \n",
      "3                0               0.684073            0     0   \n",
      "4                0               0.966908            0     0   \n",
      "..             ...                    ...          ...   ...   \n",
      "69               0               0.984761            1     1   \n",
      "70               0               0.990641            1     1   \n",
      "71               0               0.915573            1     1   \n",
      "72               0               0.979738            1     1   \n",
      "73               0               0.991341            1     1   \n",
      "\n",
      "   behavior_features_traits beliefs_expectancies  bws connotation  ...  \\\n",
      "0                       yes                  yes  NaN     neutral  ...   \n",
      "1                       yes                  yes  NaN     neutral  ...   \n",
      "2                       yes                  yes  NaN     neutral  ...   \n",
      "3            not-applicable       not-applicable  NaN     neutral  ...   \n",
      "4                       yes                  yes  NaN     neutral  ...   \n",
      "..                      ...                  ...  ...         ...  ...   \n",
      "69                      yes                  yes  NaN     neutral  ...   \n",
      "70                      yes                  yes  NaN    negative  ...   \n",
      "71                      yes                  yes  NaN     neutral  ...   \n",
      "72                      yes                  yes  NaN     neutral  ...   \n",
      "73                      yes                  yes  NaN     neutral  ...   \n",
      "\n",
      "   gram_form has_category_label  \\\n",
      "0       noun                yes   \n",
      "1       noun                yes   \n",
      "2       noun                yes   \n",
      "3       noun                yes   \n",
      "4       noun                yes   \n",
      "..       ...                ...   \n",
      "69      noun                yes   \n",
      "70      noun                yes   \n",
      "71      noun                yes   \n",
      "72      noun                yes   \n",
      "73      noun                yes   \n",
      "\n",
      "                                          information   ling_form  \\\n",
      "0                                      need to choose      subset   \n",
      "1           acts dainty when she's around her parents     generic   \n",
      "2                           likes playing with makeup  individual   \n",
      "3                                            was mean  individual   \n",
      "4   is usually the bread winner and has little to ...     generic   \n",
      "..                                                ...         ...   \n",
      "69   are unable to resist looking at people's breasts     generic   \n",
      "70  are unfaithful or dishonest about their \"true\"...     generic   \n",
      "71     own the banks and control the world's finances     generic   \n",
      "72              tend to be cruel towards the children      subset   \n",
      "73                              are just straight men     generic   \n",
      "\n",
      "   roberta_score scsc_score                 situation situation_evaluation  \\\n",
      "0       0.643247   0.476361  enduring characteristics              neutral   \n",
      "1       0.492235   0.523442     situational behaviour              neutral   \n",
      "2       0.309298   0.149133     situational behaviour             positive   \n",
      "3       0.501736   0.554778  enduring characteristics             negative   \n",
      "4       0.216560   0.530053  enduring characteristics              neutral   \n",
      "..           ...        ...                       ...                  ...   \n",
      "69      0.789663   0.744423  enduring characteristics             negative   \n",
      "70      0.915691   0.793548  enduring characteristics             negative   \n",
      "71      0.832934   0.747638  enduring characteristics             negative   \n",
      "72      0.802537   0.690731  enduring characteristics             negative   \n",
      "73      0.738471   0.747638  enduring characteristics             negative   \n",
      "\n",
      "    stereotype      target_type  \n",
      "0        False   generic target  \n",
      "1        False   generic target  \n",
      "2        False  specific target  \n",
      "3        False  specific target  \n",
      "4        False   generic target  \n",
      "..         ...              ...  \n",
      "69        True   generic target  \n",
      "70        True   generic target  \n",
      "71        True   generic target  \n",
      "72        True   generic target  \n",
      "73        True   generic target  \n",
      "\n",
      "[74 rows x 24 columns]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1a4fa28d78403292ffd4cce6ad2cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SHAP values for 74 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "PartitionExplainer explainer: 75it [00:33,  2.07it/s]                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 2/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 3/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 4/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 5/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 6/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 7/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 8/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 9/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 10/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 11/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 12/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 13/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 14/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 15/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 16/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 17/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 18/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 19/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 20/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 21/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 22/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 23/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 24/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 25/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 26/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 27/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 28/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 29/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 30/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 31/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 32/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 33/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 34/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 35/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 36/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 37/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 38/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 39/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 40/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 41/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 42/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 43/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 44/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 45/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 46/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 47/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 48/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 49/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 50/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 51/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 52/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 53/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 54/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 55/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 56/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 57/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 58/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 59/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 60/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 61/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 62/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 63/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 64/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 65/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 66/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 67/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 68/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 69/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 70/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 71/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 72/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 73/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 74/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "     sentence_id     token  value_shap  \\\n",
      "0              0      most    0.000991   \n",
      "1              0    people   -0.072308   \n",
      "2              0       who    0.064777   \n",
      "3              0       are    0.061292   \n",
      "4              0  Bisexual    0.317143   \n",
      "..           ...       ...         ...   \n",
      "968           73    people    0.304815   \n",
      "969           73       are    0.214088   \n",
      "970           73      just   -0.038183   \n",
      "971           73  straight   -0.000101   \n",
      "972           73       men    0.148798   \n",
      "\n",
      "                                          sentence  categorisation  \\\n",
      "0     most people who are Bisexual need to choose.               0   \n",
      "1     most people who are Bisexual need to choose.               0   \n",
      "2     most people who are Bisexual need to choose.               0   \n",
      "3     most people who are Bisexual need to choose.               0   \n",
      "4     most people who are Bisexual need to choose.               0   \n",
      "..                                             ...             ...   \n",
      "968  all Transgender people are just straight men.               1   \n",
      "969  all Transgender people are just straight men.               1   \n",
      "970  all Transgender people are just straight men.               1   \n",
      "971  all Transgender people are just straight men.               1   \n",
      "972  all Transgender people are just straight men.               1   \n",
      "\n",
      "     predicted_label  actual_label  \n",
      "0                  0             0  \n",
      "1                  0             0  \n",
      "2                  0             0  \n",
      "3                  0             0  \n",
      "4                  0             0  \n",
      "..               ...           ...  \n",
      "968                0             1  \n",
      "969                0             1  \n",
      "970                0             1  \n",
      "971                0             1  \n",
      "972                0             1  \n",
      "\n",
      "[973 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "sampled_data = sample_observations(file_path, k=37, seed=42)\n",
    "sampled_data.to_csv(results_path / 'sampled_data.csv')\n",
    "\n",
    "shap_results = shap_analysis(sampled_data, model_path)\n",
    "print(shap_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa8137cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing LIME values for 74 samples...\n",
      "Processing 1/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 2/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 3/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 4/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 5/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 6/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 7/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 8/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 9/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 10/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 11/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 12/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 13/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 14/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 15/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 16/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 17/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 18/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 19/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 20/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 21/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 22/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 23/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 24/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 25/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 26/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 27/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 28/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 29/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 30/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 31/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 32/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 33/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 34/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 35/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 36/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 37/74 - Categorisation: 0 - Predicted: 0 - Actual: 0\n",
      "Processing 38/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 39/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 40/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 41/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 42/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 43/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 44/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 45/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 46/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 47/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 48/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 49/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 50/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 51/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 52/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 53/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 54/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 55/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 56/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 57/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 58/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 59/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 60/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 61/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 62/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 63/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 64/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 65/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 66/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 67/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 68/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 69/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 70/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 71/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 72/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 73/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "Processing 74/74 - Categorisation: 1 - Predicted: 0 - Actual: 1\n",
      "     sentence_id     token  value_lime  \\\n",
      "0              0      most   -0.026688   \n",
      "1              0    people   -0.002559   \n",
      "2              0       who   -0.010279   \n",
      "3              0       are   -0.000426   \n",
      "4              0  Bisexual   -0.066954   \n",
      "..           ...       ...         ...   \n",
      "968           73    people   -0.028617   \n",
      "969           73       are   -0.108963   \n",
      "970           73      just   -0.088240   \n",
      "971           73  straight    0.057336   \n",
      "972           73       men    0.005153   \n",
      "\n",
      "                                          sentence  categorisation  \\\n",
      "0     most people who are Bisexual need to choose.               0   \n",
      "1     most people who are Bisexual need to choose.               0   \n",
      "2     most people who are Bisexual need to choose.               0   \n",
      "3     most people who are Bisexual need to choose.               0   \n",
      "4     most people who are Bisexual need to choose.               0   \n",
      "..                                             ...             ...   \n",
      "968  all Transgender people are just straight men.               1   \n",
      "969  all Transgender people are just straight men.               1   \n",
      "970  all Transgender people are just straight men.               1   \n",
      "971  all Transgender people are just straight men.               1   \n",
      "972  all Transgender people are just straight men.               1   \n",
      "\n",
      "     predicted_label  actual_label  \n",
      "0                  0             0  \n",
      "1                  0             0  \n",
      "2                  0             0  \n",
      "3                  0             0  \n",
      "4                  0             0  \n",
      "..               ...           ...  \n",
      "968                0             1  \n",
      "969                0             1  \n",
      "970                0             1  \n",
      "971                0             1  \n",
      "972                0             1  \n",
      "\n",
      "[973 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "lime_results = lime_analysis(sampled_data, model_path)\n",
    "print(lime_results)\n",
    "\n",
    "lime_results.to_csv(results_path / 'lime_results.csv')\n",
    "shap_results.to_csv(results_path / 'shap_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fe18179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cs/student/project_msc/2025/aisd/gracelin/gracelin/code/ai4sd/cw2/src/model_explainability/SHAP_LIME_Analysis.py:165: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  correlation, _ = pearsonr(v1, v2)\n",
      "/cs/student/project_msc/2025/aisd/gracelin/gracelin/code/ai4sd/cw2/.venv/lib/python3.10/site-packages/scipy/spatial/distance.py:1381: RuntimeWarning: invalid value encountered in divide\n",
      "  p = p / np.sum(p, axis=axis, keepdims=True)\n",
      "/cs/student/project_msc/2025/aisd/gracelin/gracelin/code/ai4sd/cw2/.venv/lib/python3.10/site-packages/scipy/spatial/distance.py:1382: RuntimeWarning: invalid value encountered in divide\n",
      "  q = q / np.sum(q, axis=axis, keepdims=True)\n",
      "/tmp/ipykernel_371520/1373441029.py:19: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sentence_similarity = merged_df.groupby('sentence_id').apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence-level similarity computed:\n",
      "   sentence_id  cosine_similarity  pearson_correlation  js_divergence\n",
      "0            0          -0.800923            -0.637900       0.457466\n",
      "1            1           0.435254             0.421570       0.330163\n",
      "2            2           0.876378             0.829210       0.212457\n",
      "3            3          -0.830929            -0.821615       0.614811\n",
      "4            4           0.631784             0.630598       0.179665\n"
     ]
    }
   ],
   "source": [
    "shap_df = pd.read_csv(results_path / 'shap_results.csv')\n",
    "lime_df = pd.read_csv(results_path / 'lime_results.csv')\n",
    "stereotype_df = pd.read_csv('../stereotype_final_all_info.csv')\n",
    "\n",
    "# Compute similarity scores by token\n",
    "token_shap = shap_df.groupby('token')['value_shap'].apply(list).reset_index()\n",
    "token_lime = lime_df.groupby('token')['value_lime'].apply(list).reset_index()\n",
    "token_merged = pd.merge(token_shap, token_lime, on='token', how='inner')\n",
    "token_merged['cosine_similarity'] = token_merged.apply(lambda row: compute_cosine_similarity(row['value_shap'], row['value_lime']), axis=1)\n",
    "token_merged['pearson_correlation'] = token_merged.apply(lambda row: compute_pearson_correlation(row['value_shap'], row['value_lime']), axis=1)\n",
    "token_merged['js_divergence'] = token_merged.apply(lambda row: compute_js_divergence(row['value_shap'], row['value_lime']), axis=1)\n",
    "token_merged.to_csv(results_path / 'token_level_similarity.csv')\n",
    "\n",
    "# Compute similarity scores by sentence\n",
    "common_columns = [col for col in shap_df.columns if col not in ['value_shap', 'value_lime', 'Unnamed: 0']]\n",
    "merged_df = pd.merge(shap_df, lime_df, on=common_columns, suffixes=('_shap', '_lime'))\n",
    "\n",
    "# Group by sentence_id and compute similarity for each sentence\n",
    "sentence_similarity = merged_df.groupby('sentence_id').apply(\n",
    "    lambda group: pd.Series({\n",
    "        'cosine_similarity': compute_cosine_similarity(group['value_shap'].tolist(), group['value_lime'].tolist()),\n",
    "        'pearson_correlation': compute_pearson_correlation(group['value_shap'].tolist(), group['value_lime'].tolist()),\n",
    "        'js_divergence': compute_js_divergence(group['value_shap'].tolist(), group['value_lime'].tolist())\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "sentence_similarity.to_csv(results_path / 'sentence_level_similarity_results.csv')\n",
    "print(\"\\nSentence-level similarity computed:\")\n",
    "print(sentence_similarity.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f36cc0a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0_shap', 'sentence_id', 'token', 'value_shap', 'sentence',\n",
       "       'categorisation', 'predicted_label', 'actual_label', 'Unnamed: 0_lime',\n",
       "       'value_lime'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "650d3b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "869af98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table\n",
    "def format_token_rankings(group, value_col='value_shap', top_n=None):\n",
    "    \"\"\"Format tokens with their values, sorted by absolute importance.\"\"\"\n",
    "    tokens_values = list(zip(group['token'], group[value_col]))\n",
    "    # Sort by absolute value (most important first)\n",
    "    tokens_values.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    if top_n:\n",
    "        tokens_values = tokens_values[:top_n]\n",
    "    return \", \".join([f'\"{t}\": {v:.3f}' for t, v in tokens_values])\n",
    "\n",
    "# Build the summary table\n",
    "summary_rows = []\n",
    "\n",
    "merged_df = pd.merge(shap_df, lime_df, on=common_columns, suffixes=('_shap', '_lime'))\n",
    "\n",
    "cols_to_use = stereotype_df.columns.difference(merged_df.columns).tolist()\n",
    "\n",
    "merged_df = pd.merge(merged_df, stereotype_df[cols_to_use], left_on='sentence', right_on='text', how='left')\n",
    "\n",
    "for sentence_id in merged_df['sentence_id'].unique():\n",
    "    sentence_group = merged_df[merged_df['sentence_id'] == sentence_id]\n",
    "    \n",
    "    # Get sentence metadata (from first row since all rows have same metadata)\n",
    "    first_row = sentence_group.iloc[0]\n",
    "    text = first_row['sentence']\n",
    "    predicted_label = first_row['predicted_label']\n",
    "    actual_label = first_row['actual_label']\n",
    "    category_indicator = first_row['generalisation_category_label']\n",
    "    connotation_indicator = first_row['connotation']\n",
    "    situation_indicator = first_row['generalisation_situation']\n",
    "    full_label = first_row['full_label']\n",
    "    information = first_row['information']\n",
    "\n",
    "    # Format token rankings (using SHAP values)\n",
    "    token_rankings = format_token_rankings(sentence_group, 'value_shap')\n",
    "    \n",
    "    # Get similarity scores\n",
    "    sim_row = sentence_similarity[sentence_similarity['sentence_id'] == sentence_id].iloc[0]\n",
    "    \n",
    "    summary_rows.append({\n",
    "        'Text Instance': text,\n",
    "        'Token Rankings': token_rankings,\n",
    "        'Full Label (SCSC)': full_label,\n",
    "        'Information (SCSC)': information,\n",
    "        'Category Indicator (SCSC)': category_indicator,\n",
    "        'Connotation Indicator (SCSC)': connotation_indicator,\n",
    "        'Situation Indicator (SCSC)': situation_indicator,\n",
    "        'Predicted Label': predicted_label,\n",
    "        'Actual Label': actual_label,\n",
    "        'Cosine Similarity': round(sim_row['cosine_similarity'], 3),\n",
    "        'Pearson R': round(sim_row['pearson_correlation'], 3),\n",
    "        'JS Divergence': round(sim_row['js_divergence'], 3)\n",
    "    })\n",
    "\n",
    "summary_table = pd.DataFrame(summary_rows)\n",
    "summary_table.to_csv(results_path / 'explanation_summary_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "785d4c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    .dataframe td {\n",
       "        white-space: normal !important;\n",
       "        word-wrap: break-word !important;\n",
       "        max-width: 300px;\n",
       "        text-align: left;\n",
       "        vertical-align: top;\n",
       "    }\n",
       "    .dataframe th {\n",
       "        text-align: center;\n",
       "        background-color: #2d3748;\n",
       "        color: white;\n",
       "    }\n",
       "    .dataframe {\n",
       "        border-collapse: collapse;\n",
       "        width: 100%;\n",
       "    }\n",
       "    .dataframe td, .dataframe th {\n",
       "        border: 1px solid #4a5568;\n",
       "        padding: 8px;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Text Instance</th>\n",
       "      <th>Token Rankings</th>\n",
       "      <th>Full Label (SCSC)</th>\n",
       "      <th>Information (SCSC)</th>\n",
       "      <th>Category Indicator (SCSC)</th>\n",
       "      <th>Connotation Indicator (SCSC)</th>\n",
       "      <th>Situation Indicator (SCSC)</th>\n",
       "      <th>Predicted Label</th>\n",
       "      <th>Actual Label</th>\n",
       "      <th>Cosine Similarity</th>\n",
       "      <th>Pearson R</th>\n",
       "      <th>JS Divergence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>He likes to stay in after long work weeks. Allen is kind of an alpha male and a playboy.</td>\n",
       "      <td>\"of\": 0.140, \"kind\": -0.127, \"male\": 0.124, \"Allen\": -0.099, \"and\": 0.078, \"playboy\": -0.076, \"an\": 0.049, \"a\": 0.047, \"is\": -0.039, \"likes\": -0.031, \"alpha\": 0.031, \"work\": -0.019, \"long\": 0.006, \"He\": -0.005, \"to\": 0.005, \"after\": 0.004, \"stay\": -0.003, \"weeks\": -0.002, \"in\": -0.001</td>\n",
       "      <td>an alpha male</td>\n",
       "      <td>is kind of an alpha male and a playboy</td>\n",
       "      <td>subset_generic target</td>\n",
       "      <td>neutral</td>\n",
       "      <td>enduring characteristics_abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>They call her a girl for several reasons.</td>\n",
       "      <td>\"reasons\": 0.285, \"They\": 0.248, \"call\": -0.139, \"a\": 0.128, \"girl\": -0.081, \"several\": -0.061, \"for\": 0.029, \"her\": -0.016</td>\n",
       "      <td>a girl</td>\n",
       "      <td>not-applicable</td>\n",
       "      <td>individual_specific target</td>\n",
       "      <td>neutral</td>\n",
       "      <td>not-applicable_not-applicable</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>South Africa hosted the 1995 Rugby World Cup. Mostly the rich and powerful whites play rugby.</td>\n",
       "      <td>\"whites\": 0.157, \"powerful\": -0.107, \"rugby\": -0.058, \"and\": 0.035, \"rich\": -0.020, \"Mostly\": -0.017, \"play\": -0.011, \"Cup\": -0.010, \"South\": 0.007, \"the\": 0.006, \"the\": 0.006, \"hosted\": -0.005, \"Rugby\": -0.002, \"World\": 0.002, \"Africa\": 0.001, \"1995\": 0.001, \"the\": -0.001, \"the\": -0.001</td>\n",
       "      <td>the rich and powerful whites</td>\n",
       "      <td>play rugby</td>\n",
       "      <td>subset_generic target</td>\n",
       "      <td>neutral</td>\n",
       "      <td>enduring characteristics_concrete</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>The girl's strong boyfriend ordered them some drinks.</td>\n",
       "      <td>\"some\": -0.041, \"them\": 0.037, \"girl\": -0.017, \"strong\": -0.014, \"The\": 0.005, \"boyfriend\": -0.004, \"s\": 0.003, \"drinks\": -0.002, \"ordered\": 0.000</td>\n",
       "      <td>The girl</td>\n",
       "      <td>has a strong boyfriend</td>\n",
       "      <td>subset_specific target</td>\n",
       "      <td>neutral</td>\n",
       "      <td>enduring characteristics_concrete</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>His mother was a very nurturing woman.</td>\n",
       "      <td>\"woman\": 0.113, \"nurturing\": -0.083, \"very\": -0.066, \"mother\": -0.044, \"a\": 0.033, \"was\": -0.020, \"His\": 0.016</td>\n",
       "      <td>woman</td>\n",
       "      <td>was a very nurturing</td>\n",
       "      <td>generic_specific target</td>\n",
       "      <td>neutral</td>\n",
       "      <td>enduring characteristics_concrete</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary_table = pd.read_csv(results_path / 'explanation_summary_table.csv')\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Display with HTML styling for text wrapping\n",
    "styled_html = \"\"\"\n",
    "<style>\n",
    "    .dataframe td {\n",
    "        white-space: normal !important;\n",
    "        word-wrap: break-word !important;\n",
    "        max-width: 300px;\n",
    "        text-align: left;\n",
    "        vertical-align: top;\n",
    "    }\n",
    "    .dataframe th {\n",
    "        text-align: center;\n",
    "        background-color: #2d3748;\n",
    "        color: white;\n",
    "    }\n",
    "    .dataframe {\n",
    "        border-collapse: collapse;\n",
    "        width: 100%;\n",
    "    }\n",
    "    .dataframe td, .dataframe th {\n",
    "        border: 1px solid #4a5568;\n",
    "        padding: 8px;\n",
    "    }\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "correct_predictions = summary_table[summary_table['Predicted Label'] == summary_table['Actual Label']]\n",
    "\n",
    "# Display a sample of 5 rows\n",
    "display(HTML(styled_html + correct_predictions.sample(5).to_html(index=False, escape=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c6624d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    .dataframe td {\n",
       "        white-space: normal !important;\n",
       "        word-wrap: break-word !important;\n",
       "        max-width: 300px;\n",
       "        text-align: left;\n",
       "        vertical-align: top;\n",
       "    }\n",
       "    .dataframe th {\n",
       "        text-align: center;\n",
       "        background-color: #2d3748;\n",
       "        color: white;\n",
       "    }\n",
       "    .dataframe {\n",
       "        border-collapse: collapse;\n",
       "        width: 100%;\n",
       "    }\n",
       "    .dataframe td, .dataframe th {\n",
       "        border: 1px solid #4a5568;\n",
       "        padding: 8px;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Text Instance</th>\n",
       "      <th>Predicted Label</th>\n",
       "      <th>Actual Label</th>\n",
       "      <th>Token Rankings</th>\n",
       "      <th>Category Indicator (SCSC)</th>\n",
       "      <th>Connotation Indicator (SCSC)</th>\n",
       "      <th>Situation Indicator (SCSC)</th>\n",
       "      <th>Full Label (SCSC)</th>\n",
       "      <th>Information (SCSC)</th>\n",
       "      <th>Cosine Similarity</th>\n",
       "      <th>Pearson R</th>\n",
       "      <th>JS Divergence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>My male friend is here. He is very masculine and strong.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>\"strong\": 0.304, \"masculine\": 0.227, \"male\": 0.167, \"friend\": -0.056, \"He\": 0.046, \"very\": 0.037, \"is\": 0.032, \"is\": 0.032, \"is\": 0.030, \"is\": 0.030, \"here\": -0.012, \"My\": -0.011, \"and\": -0.005</td>\n",
       "      <td>individual_specific target</td>\n",
       "      <td>neutral</td>\n",
       "      <td>enduring characteristics_concrete</td>\n",
       "      <td>My male friend</td>\n",
       "      <td>is very masculine and strong</td>\n",
       "      <td>-0.479</td>\n",
       "      <td>-0.183</td>\n",
       "      <td>0.493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>I fell for a man from India and he took advantage of me. He had dark hair and a strong accent.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>\"accent\": 0.248, \"hair\": -0.210, \"had\": 0.179, \"dark\": 0.161, \"strong\": 0.141, \"and\": 0.076, \"and\": 0.076, \"I\": -0.067, \"me\": 0.062, \"He\": 0.060, \"fell\": -0.042, \"for\": 0.041, \"man\": 0.040, \"he\": 0.040, \"of\": 0.031, \"took\": 0.020, \"India\": 0.017, \"a\": -0.012, \"a\": -0.012, \"advantage\": 0.007, \"a\": -0.005, \"a\": -0.005, \"from\": 0.004, \"and\": 0.004, \"and\": 0.004</td>\n",
       "      <td>individual_specific target</td>\n",
       "      <td>neutral</td>\n",
       "      <td>situational behaviour_concrete</td>\n",
       "      <td>a man from India</td>\n",
       "      <td>took advantage of me</td>\n",
       "      <td>-0.553</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>0.257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Indians partake in a outdated way of life.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Indians\": 0.274, \"partake\": -0.236, \"outdated\": 0.194, \"of\": 0.122, \"in\": -0.111, \"way\": 0.089, \"a\": 0.073, \"life\": -0.013</td>\n",
       "      <td>generic_generic target</td>\n",
       "      <td>negative</td>\n",
       "      <td>enduring characteristics_abstract</td>\n",
       "      <td>Indians</td>\n",
       "      <td>partake in a outdated way of life</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>The Chinese were superstitious and believed in lucky numbers</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>\"superstitious\": 0.321, \"in\": -0.101, \"lucky\": 0.101, \"Chinese\": -0.052, \"and\": 0.052, \"believed\": 0.032, \"numbers\": -0.024, \"were\": -0.016, \"The\": 0.002</td>\n",
       "      <td>generic_generic target</td>\n",
       "      <td>neutral</td>\n",
       "      <td>enduring characteristics_abstract</td>\n",
       "      <td>The Chinese</td>\n",
       "      <td>were superstitious and believed in lucky numbers</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Those from Africa are black. They are fast runners.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>\"fast\": 0.329, \"They\": 0.191, \"black\": 0.179, \"runners\": -0.126, \"are\": -0.034, \"are\": -0.034, \"from\": -0.033, \"are\": 0.031, \"are\": 0.031, \"Africa\": 0.009, \"Those\": -0.007</td>\n",
       "      <td>subset_generic target</td>\n",
       "      <td>neutral</td>\n",
       "      <td>enduring characteristics_concrete</td>\n",
       "      <td>Those from Africa</td>\n",
       "      <td>are black, are fast runners</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "incorrect_predictions = summary_table[summary_table['Predicted Label'] != summary_table['Actual Label']]\n",
    "\n",
    "# Display a sample of 5 rows\n",
    "display(HTML(styled_html + incorrect_predictions.sample(5).to_html(index=False, escape=False)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
