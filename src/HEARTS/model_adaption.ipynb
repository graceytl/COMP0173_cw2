{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53216c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cs/student/project_msc/2025/aisd/gracelin/gracelin/code/ai4sd/cw2/src/HEARTS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ab07ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4070 Ti SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49359a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "os.environ[\"HUGGINGFACE_TRAINER_ENABLE_PROGRESS_BAR\"] = \"1\"\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "478dfc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from codecarbon import EmissionsTracker\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, pipeline\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6869d6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc524c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"albert/albert-base-v2\"\n",
    "model_output_dir = \"custom_albertv2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaed2817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification: stereotype vs non-stereotype\n",
    "# Map labels containing \"stereotype\" to 1, all others to 0\n",
    "label2id = {\"False\": 0, \"True\": 1}\n",
    "id2label = {0: \"False\", 1: \"True\"}\n",
    "num_labels = 2\n",
    "\n",
    "# Convert original labels to binary\n",
    "def get_binary_label(label):\n",
    "    return 1 if label else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3a15002",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_df = pd.read_csv(\"../stereotype_final.csv\", index_col=False)\n",
    "custom_df['category'] = custom_df['stereotype'].map(lambda x: 1 if x else 0)\n",
    "custom_df.rename(columns={'sentence': 'text'}, inplace=True)\n",
    "\n",
    "custom_df, test_df = train_test_split(custom_df, test_size = 0.10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "286f01d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'generalisation_category_label', 'connotation',\n",
       "       'gram_form', 'generalisation_situation', 'situation_evaluation', 'text',\n",
       "       'scsc_score', 'stereotype', 'roberta_score', 'category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a537cda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 12:11:03] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 12:11:03] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 12:11:03] [setup] CPU Tracking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 12:11:03] \tRAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:0/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*\n",
      "[codecarbon WARNING @ 12:11:03] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 12:11:03] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-9800X CPU @ 3.80GHz\n",
      "[codecarbon WARNING @ 12:11:03] No CPU tracking mode found. Falling back on CPU load mode.\n",
      "[codecarbon INFO @ 12:11:03] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 12:11:03] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 12:11:03] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: cpu_load\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 12:11:03] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 12:11:03]   Platform system: Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34\n",
      "[codecarbon INFO @ 12:11:03]   Python version: 3.10.19\n",
      "[codecarbon INFO @ 12:11:03]   CodeCarbon version: 3.2.0\n",
      "[codecarbon INFO @ 12:11:03]   Available RAM : 30.799 GB\n",
      "[codecarbon INFO @ 12:11:03]   CPU count: 16 thread(s) in 1 physical CPU(s)\n",
      "[codecarbon INFO @ 12:11:03]   CPU model: Intel(R) Core(TM) i7-9800X CPU @ 3.80GHz\n",
      "[codecarbon INFO @ 12:11:03]   GPU count: 1\n",
      "[codecarbon INFO @ 12:11:03]   GPU model: 1 x NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "[codecarbon INFO @ 12:11:07] Emissions data (if any) will be saved to file /cs/student/project_msc/2025/aisd/gracelin/gracelin/code/ai4sd/cw2/src/HEARTS/emissions.csv\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input from train: {'Unnamed: 0': 234, 'generalisation_category_label': 'subset_generic target', 'connotation': 'neutral', 'gram_form': 'noun', 'generalisation_situation': 'enduring characteristics_abstract', 'situation_evaluation': 'negative', 'text': 'People living in syria just want to be violent', 'scsc_score': 0.7465039163141961, 'stereotype': True, 'roberta_score': 0.7847822, 'category': 1, '__index_level_0__': 234}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190a368f3f3542099da6694c0083561c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e112a246bc4638b1afd6177ca01b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb5842bfe094ca2ae22c44f2f353ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/712 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcab6a7522d44b4d940fb4b778fcea87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/712 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from train: {'input_ids': [2, 17044, 25, 21, 476, 16, 2257, 3860, 148, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n",
      "Sample tokenized input from validation: {'input_ids': [2, 14, 9148, 2014, 1687, 491, 57, 289, 24, 23, 228, 20, 233, 2230, 875, 34, 4721, 16435, 4894, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}\n",
      "Label type check (train): <class 'int'> = 0\n",
      "Label type check (val): <class 'int'> = 1\n",
      "Columns in tokenized_train: ['input_ids', 'token_type_ids', 'attention_mask', 'labels']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 12:11:09] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 12:11:09] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 12:11:09] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 12:11:10] \tRAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:0/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*\n",
      "[codecarbon WARNING @ 12:11:10] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 12:11:10] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-9800X CPU @ 3.80GHz\n",
      "[codecarbon WARNING @ 12:11:10] No CPU tracking mode found. Falling back on CPU load mode.\n",
      "[codecarbon INFO @ 12:11:10] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 12:11:10] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 12:11:10] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: cpu_load\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 12:11:10] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 12:11:10]   Platform system: Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34\n",
      "[codecarbon INFO @ 12:11:10]   Python version: 3.10.19\n",
      "[codecarbon INFO @ 12:11:10]   CodeCarbon version: 3.2.0\n",
      "[codecarbon INFO @ 12:11:10]   Available RAM : 30.799 GB\n",
      "[codecarbon INFO @ 12:11:10]   CPU count: 16 thread(s) in 1 physical CPU(s)\n",
      "[codecarbon INFO @ 12:11:10]   CPU model: Intel(R) Core(TM) i7-9800X CPU @ 3.80GHz\n",
      "[codecarbon INFO @ 12:11:10]   GPU count: 1\n",
      "[codecarbon INFO @ 12:11:10]   GPU model: 1 x NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "[codecarbon INFO @ 12:11:13] Emissions data (if any) will be saved to file /cs/student/project_msc/2025/aisd/gracelin/gracelin/code/ai4sd/cw2/src/HEARTS/custom_albertv2/emissions.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='270' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [270/270 01:22, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.505904</td>\n",
       "      <td>0.762470</td>\n",
       "      <td>0.765895</td>\n",
       "      <td>0.760690</td>\n",
       "      <td>0.765895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.410425</td>\n",
       "      <td>0.830849</td>\n",
       "      <td>0.809888</td>\n",
       "      <td>0.814562</td>\n",
       "      <td>0.809888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.410081</td>\n",
       "      <td>0.824248</td>\n",
       "      <td>0.821862</td>\n",
       "      <td>0.822886</td>\n",
       "      <td>0.821862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.404072</td>\n",
       "      <td>0.833413</td>\n",
       "      <td>0.833629</td>\n",
       "      <td>0.833520</td>\n",
       "      <td>0.833629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.402253</td>\n",
       "      <td>0.839704</td>\n",
       "      <td>0.838011</td>\n",
       "      <td>0.838773</td>\n",
       "      <td>0.838011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.431985</td>\n",
       "      <td>0.833079</td>\n",
       "      <td>0.830075</td>\n",
       "      <td>0.831330</td>\n",
       "      <td>0.830075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 12:11:22] Energy consumed for RAM : 0.000086 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 12:11:23] Delta energy consumed for CPU with cpu_load : 0.000085 kWh, power : 19.74569610090001 W\n",
      "[codecarbon INFO @ 12:11:23] Energy consumed for All CPU : 0.000085 kWh\n",
      "[codecarbon INFO @ 12:11:23] Energy consumed for all GPUs : 0.000737 kWh. Total GPU Power : 165.7024668344477 W\n",
      "[codecarbon INFO @ 12:11:23] 0.000908 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 12:11:29] Energy consumed for RAM : 0.000089 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 12:11:30] Delta energy consumed for CPU with cpu_load : 0.000079 kWh, power : 17.624555701500004 W\n",
      "[codecarbon INFO @ 12:11:30] Energy consumed for All CPU : 0.000079 kWh\n",
      "[codecarbon INFO @ 12:11:30] Energy consumed for all GPUs : 0.000946 kWh. Total GPU Power : 195.81271363108314 W\n",
      "[codecarbon INFO @ 12:11:30] 0.001114 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 12:11:37] Energy consumed for RAM : 0.000167 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 12:11:38] Delta energy consumed for CPU with cpu_load : 0.000077 kWh, power : 19.084608988500005 W\n",
      "[codecarbon INFO @ 12:11:38] Energy consumed for All CPU : 0.000162 kWh\n",
      "[codecarbon INFO @ 12:11:38] Energy consumed for all GPUs : 0.001633 kWh. Total GPU Power : 215.1367272424515 W\n",
      "[codecarbon INFO @ 12:11:38] 0.001962 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 12:11:44] Energy consumed for RAM : 0.000167 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 12:11:45] Delta energy consumed for CPU with cpu_load : 0.000068 kWh, power : 17.354968593600006 W\n",
      "[codecarbon INFO @ 12:11:45] Energy consumed for All CPU : 0.000146 kWh\n",
      "[codecarbon INFO @ 12:11:45] Energy consumed for all GPUs : 0.001799 kWh. Total GPU Power : 211.3159378255622 W\n",
      "[codecarbon INFO @ 12:11:45] 0.002112 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 12:11:52] Energy consumed for RAM : 0.000247 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 12:11:53] Delta energy consumed for CPU with cpu_load : 0.000069 kWh, power : 17.1907161195 W\n",
      "[codecarbon INFO @ 12:11:53] Energy consumed for All CPU : 0.000231 kWh\n",
      "[codecarbon INFO @ 12:11:53] Energy consumed for all GPUs : 0.002510 kWh. Total GPU Power : 210.68769068235298 W\n",
      "[codecarbon INFO @ 12:11:53] 0.002989 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 12:11:59] Energy consumed for RAM : 0.000247 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 12:12:00] Delta energy consumed for CPU with cpu_load : 0.000070 kWh, power : 17.40056194682143 W\n",
      "[codecarbon INFO @ 12:12:00] Energy consumed for All CPU : 0.000216 kWh\n",
      "[codecarbon INFO @ 12:12:00] Energy consumed for all GPUs : 0.002654 kWh. Total GPU Power : 205.45808078900757 W\n",
      "[codecarbon INFO @ 12:12:00] 0.003118 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 12:12:07] Energy consumed for RAM : 0.000328 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 12:12:08] Delta energy consumed for CPU with cpu_load : 0.000071 kWh, power : 17.7289868712 W\n",
      "[codecarbon INFO @ 12:12:08] Energy consumed for All CPU : 0.000302 kWh\n",
      "[codecarbon INFO @ 12:12:08] Energy consumed for all GPUs : 0.003363 kWh. Total GPU Power : 204.6006314678847 W\n",
      "[codecarbon INFO @ 12:12:08] 0.003993 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 12:12:14] Energy consumed for RAM : 0.000328 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 12:12:15] Delta energy consumed for CPU with cpu_load : 0.000069 kWh, power : 17.229039245343753 W\n",
      "[codecarbon INFO @ 12:12:15] Energy consumed for All CPU : 0.000286 kWh\n",
      "[codecarbon INFO @ 12:12:15] Energy consumed for all GPUs : 0.003553 kWh. Total GPU Power : 215.63779398774378 W\n",
      "[codecarbon INFO @ 12:12:15] 0.004166 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 12:12:22] Energy consumed for RAM : 0.000408 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 12:12:23] Delta energy consumed for CPU with cpu_load : 0.000069 kWh, power : 17.142505529531256 W\n",
      "[codecarbon INFO @ 12:12:23] Energy consumed for All CPU : 0.000371 kWh\n",
      "[codecarbon INFO @ 12:12:23] Energy consumed for all GPUs : 0.004264 kWh. Total GPU Power : 216.34918562070195 W\n",
      "[codecarbon INFO @ 12:12:23] 0.005044 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 12:12:29] Energy consumed for RAM : 0.000408 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 12:12:30] Delta energy consumed for CPU with cpu_load : 0.000069 kWh, power : 17.164020413718752 W\n",
      "[codecarbon INFO @ 12:12:30] Energy consumed for All CPU : 0.000355 kWh\n",
      "[codecarbon INFO @ 12:12:30] Energy consumed for all GPUs : 0.004447 kWh. Total GPU Power : 214.81850285724406 W\n",
      "[codecarbon INFO @ 12:12:30] 0.005211 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 12:12:38] Energy consumed for RAM : 0.000493 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 12:12:39] Delta energy consumed for CPU with cpu_load : 0.000073 kWh, power : 17.16184363138236 W\n",
      "[codecarbon INFO @ 12:12:39] Energy consumed for All CPU : 0.000444 kWh\n",
      "[codecarbon INFO @ 12:12:39] Energy consumed for all GPUs : 0.005215 kWh. Total GPU Power : 208.62874523701728 W\n",
      "[codecarbon INFO @ 12:12:39] 0.006153 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 12:12:39] Energy consumed for RAM : 0.000460 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 12:12:40] Delta energy consumed for CPU with cpu_load : 0.000044 kWh, power : 17.1490292265 W\n",
      "[codecarbon INFO @ 12:12:40] Energy consumed for All CPU : 0.000399 kWh\n",
      "[codecarbon INFO @ 12:12:40] Energy consumed for all GPUs : 0.005017 kWh. Total GPU Power : 209.4891959926148 W\n",
      "[codecarbon INFO @ 12:12:40] 0.005876 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 12:12:41] Energy consumed for RAM : 0.000503 kWh. RAM Power : 20.0 W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 12:12:41] Delta energy consumed for CPU with cpu_load : 0.000008 kWh, power : 16.554860602500003 W\n",
      "[codecarbon INFO @ 12:12:41] Energy consumed for All CPU : 0.000452 kWh\n",
      "[codecarbon INFO @ 12:12:41] Energy consumed for all GPUs : 0.005278 kWh. Total GPU Power : 101.16225085754458 W\n",
      "[codecarbon INFO @ 12:12:41] 0.006233 kWh of electricity and 0.000000 L of water were used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total emissions: 0.0014809582398911231 kg CO2\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(88)\n",
    "\n",
    "tracker = EmissionsTracker()\n",
    "tracker.start()\n",
    "\n",
    "try:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=num_labels, \n",
    "        id2label=id2label, label2id=label2id, \n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    custom_df = ds.from_pandas(custom_df)\n",
    "\n",
    "    custom_train_split, custom_val = custom_df.train_test_split(test_size=0.2, seed=88).values()\n",
    "\n",
    "    print(\"Sample input from train:\", custom_df[0])\n",
    "\n",
    "    tokenized_train = custom_train_split.map(tokenize_function, batched=True).map(\n",
    "        lambda examples: {'labels': get_binary_label(examples['category'])}\n",
    "    )\n",
    "    tokenized_val = custom_val.map(tokenize_function, batched=True).map(\n",
    "        lambda examples: {'labels': get_binary_label(examples['category'])}\n",
    "    )\n",
    "    \n",
    "    # Remove columns that can't be converted to tensors (keep only what the model needs)\n",
    "    columns_to_remove = [col for col in tokenized_train.column_names if col not in ['input_ids', 'attention_mask', 'token_type_ids', 'labels']]\n",
    "    tokenized_train = tokenized_train.remove_columns(columns_to_remove)\n",
    "    tokenized_val = tokenized_val.remove_columns(columns_to_remove)\n",
    "    \n",
    "    print(\"Sample tokenized input from train:\", tokenized_train[0])\n",
    "    print(\"Sample tokenized input from validation:\", tokenized_val[0])\n",
    "    print(f\"Label type check (train): {type(tokenized_train[0]['labels'])} = {tokenized_train[0]['labels']}\")\n",
    "    print(f\"Label type check (val): {type(tokenized_val[0]['labels'])} = {tokenized_val[0]['labels']}\")\n",
    "    print(f\"Columns in tokenized_train: {tokenized_train.column_names}\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
    "        balanced_acc = balanced_accuracy_score(labels, predictions)\n",
    "        return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"balanced accuracy\": balanced_acc}\n",
    "\n",
    "    model_output_dir_path = Path(model_output_dir)\n",
    "    model_output_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_output_dir_path, num_train_epochs=6, eval_strategy=\"epoch\", learning_rate=2e-5,\n",
    "        per_device_train_batch_size=64, per_device_eval_batch_size=64, weight_decay=0.01,\n",
    "        save_strategy=\"epoch\", load_best_model_at_end=True, save_total_limit=1)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, args=training_args, processing_class=tokenizer, train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val, compute_metrics=compute_metrics)\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(model_output_dir_path)\n",
    "    print(\"Training complete.\")\n",
    "finally:\n",
    "    emissions: float = tracker.stop()\n",
    "\n",
    "print(f\"Estimated total emissions: {str(emissions)} kg CO2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea1b2985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 12:17:42] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 12:17:42] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 12:17:42] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 12:17:43] \tRAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:0/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*\n",
      "[codecarbon WARNING @ 12:17:43] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 12:17:43] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-9800X CPU @ 3.80GHz\n",
      "[codecarbon WARNING @ 12:17:43] No CPU tracking mode found. Falling back on CPU load mode.\n",
      "[codecarbon INFO @ 12:17:43] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 12:17:43] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 12:17:43] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: cpu_load\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 12:17:43] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 12:17:43]   Platform system: Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34\n",
      "[codecarbon INFO @ 12:17:43]   Python version: 3.10.19\n",
      "[codecarbon INFO @ 12:17:43]   CodeCarbon version: 3.2.0\n",
      "[codecarbon INFO @ 12:17:43]   Available RAM : 30.799 GB\n",
      "[codecarbon INFO @ 12:17:43]   CPU count: 16 thread(s) in 1 physical CPU(s)\n",
      "[codecarbon INFO @ 12:17:43]   CPU model: Intel(R) Core(TM) i7-9800X CPU @ 3.80GHz\n",
      "[codecarbon INFO @ 12:17:43]   GPU count: 1\n",
      "[codecarbon INFO @ 12:17:43]   GPU model: 1 x NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "[codecarbon INFO @ 12:17:46] Emissions data (if any) will be saved to file /cs/student/project_msc/2025/aisd/gracelin/gracelin/code/ai4sd/cw2/src/HEARTS/emissions.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89489a0f00cd4ff48a7b939c3ed95307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/396 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eed95afb16249cb87d74185d5029240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/396 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'Unnamed: 0': 1459, 'generalisation_category_label': 'subset_generic target', 'connotation': 'negative', 'gram_form': 'noun', 'generalisation_situation': 'enduring characteristics_abstract', 'situation_evaluation': 'negative', 'text': 'Qatar is nice around this time of year. That place is terrorist just like every other Arab country.', 'scsc_score': 0.7366418006826077, 'stereotype': True, 'roberta_score': 0.7959243, 'category': 1, '__index_level_0__': 1459, 'input_ids': [2, 12384, 25, 2210, 140, 48, 85, 16, 159, 9, 30, 209, 25, 10012, 114, 101, 352, 89, 3666, 475, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 12:17:51] Energy consumed for RAM : 0.000028 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 12:17:52] Delta energy consumed for CPU with cpu_load : 0.000026 kWh, power : 18.2777283678 W\n",
      "[codecarbon INFO @ 12:17:52] Energy consumed for All CPU : 0.000026 kWh\n",
      "[codecarbon INFO @ 12:17:52] Energy consumed for all GPUs : 0.000186 kWh. Total GPU Power : 119.2919555880455 W\n",
      "[codecarbon INFO @ 12:17:52] 0.000241 kWh of electricity and 0.000000 L of water were used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total emissions: 5.721295564169141e-05 kg CO2\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(88)\n",
    "\n",
    "tracker = EmissionsTracker()\n",
    "tracker.start()\n",
    "\n",
    "try:\n",
    "    test_df = ds.from_pandas(test_df)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_output_dir,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id, \n",
    "        ignore_mismatched_sizes=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_output_dir)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    tokenized_test = test_df.map(tokenize_function, batched=True).map(\n",
    "        lambda examples: {'labels': get_binary_label(examples['category'])})\n",
    "    print(\"Sample tokenized input from test:\", tokenized_test[0])\n",
    "\n",
    "    result_output_dir = Path(model_output_dir).parent / \"custom_results\"\n",
    "    result_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Use GPU (device=0) instead of CPU (device=-1) for faster inference\n",
    "    pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "    # Convert to list - the pipeline expects a list of strings\n",
    "    test_texts = list(test_df['text'])\n",
    "    predictions = pipe(test_texts, top_k=1)\n",
    "\n",
    "    # Extract label and score from nested list results\n",
    "    pred_labels = [1 if pred[0]['label'] == 'stereotype' else 0 for pred in predictions]\n",
    "    pred_probs = [pred[0]['score'] for pred in predictions]\n",
    "    y_true = [get_binary_label(label) for label in test_df['category']]\n",
    "    results_df = pd.DataFrame({\n",
    "        'text': test_df['text'],\n",
    "        'predicted_label': pred_labels,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'actual_label': y_true,\n",
    "        'group': test_df['category'],\n",
    "    })\n",
    "\n",
    "    results_file_path = result_output_dir / \"full_results.csv\"\n",
    "    results_df.to_csv(results_file_path, index=False)\n",
    "finally:\n",
    "    emissions: float = tracker.stop()\n",
    "\n",
    "print(f\"Estimated total emissions: {str(emissions)} kg CO2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
