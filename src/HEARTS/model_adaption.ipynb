{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53216c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cs/student/project_msc/2025/aisd/gracelin/gracelin/code/ai4sd/cw2/src/HEARTS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ab07ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4070 Ti SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49359a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "os.environ[\"HUGGINGFACE_TRAINER_ENABLE_PROGRESS_BAR\"] = \"1\"\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "478dfc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from codecarbon import EmissionsTracker\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, pipeline\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6869d6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc524c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"albert/albert-base-v2\"\n",
    "model_output_dir = \"albertv2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaed2817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification: stereotype vs non-stereotype\n",
    "# Map labels containing \"stereotype\" to 1, all others to 0\n",
    "label2id = {\"non-stereotype\": 0, \"stereotype\": 1}\n",
    "id2label = {0: \"non-stereotype\", 1: \"stereotype\"}\n",
    "num_labels = 2\n",
    "\n",
    "# Convert original labels to binary\n",
    "def get_binary_label(label):\n",
    "    return 1 if label else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3a15002",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_df = pd.read_csv(\"../stereotype_final.csv\")\n",
    "custom_df['category'] = custom_df['stereotype'].map(lambda x: 1 if x else 0)\n",
    "\n",
    "custom_df = ds.from_pandas(custom_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea1b2985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 19:34:22] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 19:34:22] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 19:34:22] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 19:34:22] \tRAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:0/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*\n",
      "[codecarbon WARNING @ 19:34:22] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 19:34:22] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-9800X CPU @ 3.80GHz\n",
      "[codecarbon WARNING @ 19:34:22] No CPU tracking mode found. Falling back on CPU load mode.\n",
      "[codecarbon INFO @ 19:34:22] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 19:34:22] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 19:34:22] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: cpu_load\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 19:34:22] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 19:34:22]   Platform system: Linux-5.14.0-570.58.1.el9_6.x86_64-x86_64-with-glibc2.34\n",
      "[codecarbon INFO @ 19:34:22]   Python version: 3.10.19\n",
      "[codecarbon INFO @ 19:34:22]   CodeCarbon version: 3.2.0\n",
      "[codecarbon INFO @ 19:34:22]   Available RAM : 30.799 GB\n",
      "[codecarbon INFO @ 19:34:22]   CPU count: 16 thread(s) in 1 physical CPU(s)\n",
      "[codecarbon INFO @ 19:34:22]   CPU model: Intel(R) Core(TM) i7-9800X CPU @ 3.80GHz\n",
      "[codecarbon INFO @ 19:34:22]   GPU count: 1\n",
      "[codecarbon INFO @ 19:34:22]   GPU model: 1 x NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "[codecarbon INFO @ 19:34:25] Emissions data (if any) will be saved to file /cs/student/project_msc/2025/aisd/gracelin/gracelin/code/ai4sd/cw2/src/HEARTS/emissions.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf8ab40f3e649ba9a0bf78419bbf4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3953 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db60971a959e462689656fca8ad5d737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3953 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input from test: {'Unnamed: 0': 0, 'generalisation_category_label': 'generic_generic target', 'connotation': 'negative', 'gram_form': 'noun', 'generalisation_situation': 'enduring characteristics_abstract', 'situation_evaluation': 'negative', 'sentence': 'Bangladesh people are lous and quarell they are not quite.', 'scsc_score': 0.859226301874753, 'stereotype': True, 'roberta_score': 0.68846756, 'category': 1, 'input_ids': [2, 5919, 148, 50, 6118, 18, 17, 7131, 10451, 59, 50, 52, 1450, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 19:34:41] Energy consumed for RAM : 0.000086 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 19:34:41] Delta energy consumed for CPU with cpu_load : 0.000071 kWh, power : 16.548473686799998 W\n",
      "[codecarbon INFO @ 19:34:41] Energy consumed for All CPU : 0.000071 kWh\n",
      "[codecarbon INFO @ 19:34:41] Energy consumed for all GPUs : 0.000277 kWh. Total GPU Power : 62.3081221821576 W\n",
      "[codecarbon INFO @ 19:34:41] 0.000435 kWh of electricity and 0.000000 L of water were used since the beginning.\n",
      "[codecarbon INFO @ 19:34:55] Energy consumed for RAM : 0.000159 kWh. RAM Power : 20.0 W\n",
      "[codecarbon INFO @ 19:34:55] Delta energy consumed for CPU with cpu_load : 0.000061 kWh, power : 16.5380549763 W\n",
      "[codecarbon INFO @ 19:34:55] Energy consumed for All CPU : 0.000132 kWh\n",
      "[codecarbon INFO @ 19:34:55] Energy consumed for all GPUs : 0.000538 kWh. Total GPU Power : 68.51328228705712 W\n",
      "[codecarbon INFO @ 19:34:55] 0.000829 kWh of electricity and 0.000000 L of water were used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total emissions: 0.00019698314037222951 kg CO2\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(88)\n",
    "\n",
    "tracker = EmissionsTracker()\n",
    "tracker.start()\n",
    "\n",
    "try:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_output_dir,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id, \n",
    "        ignore_mismatched_sizes=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_output_dir)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"sentence\"], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    tokenized_test = custom_df.map(tokenize_function, batched=True).map(\n",
    "        lambda examples: {'labels': get_binary_label(examples['category'])})\n",
    "    print(\"Sample tokenized input from test:\", tokenized_test[0])\n",
    "\n",
    "    result_output_dir = Path(model_output_dir).parent / \"custom_results\"\n",
    "    result_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Use GPU (device=0) instead of CPU (device=-1) for faster inference\n",
    "    pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "    # Convert to list - the pipeline expects a list of strings\n",
    "    test_texts = list(custom_df['sentence'])\n",
    "    predictions = pipe(test_texts, top_k=1)\n",
    "\n",
    "    # Extract label and score from nested list results\n",
    "    pred_labels = [1 if pred[0]['label'] == 'stereotype' else 0 for pred in predictions]\n",
    "    pred_probs = [pred[0]['score'] for pred in predictions]\n",
    "    y_true = [get_binary_label(label) for label in custom_df['category']]\n",
    "    results_df = pd.DataFrame({\n",
    "        'text': custom_df['sentence'],\n",
    "        'predicted_label': pred_labels,\n",
    "        'predicted_probability': pred_probs,\n",
    "        'actual_label': y_true,\n",
    "        'group': custom_df['category'],\n",
    "    })\n",
    "\n",
    "    results_file_path = result_output_dir / \"full_results.csv\"\n",
    "    results_df.to_csv(results_file_path, index=False)\n",
    "finally:\n",
    "    emissions: float = tracker.stop()\n",
    "\n",
    "print(f\"Estimated total emissions: {str(emissions)} kg CO2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
